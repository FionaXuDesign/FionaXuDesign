{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FionaXuDesign/FionaXuDesign/blob/main/Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUv8giYyh1F2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25a0753e-fd2f-44f1-c162-18733813d770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.11\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch>=2.0.0 torchvision>=0.15.0 numpy>=1.21.6 Pillow>=9.4.0"
      ],
      "metadata": {
        "id": "4BFoiAsnilvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# ---------------------- 数据预处理 ----------------------\n",
        "class FFT:\n",
        "    def __init__(self, image_size):\n",
        "        self.image_size = image_size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        try:\n",
        "            if not isinstance(img, Image.Image):\n",
        "                img = transforms.ToPILImage()(img)\n",
        "            img_gray = img.convert('L').resize((self.image_size, self.image_size))\n",
        "            gray_np = np.array(img_gray, dtype=np.float32)\n",
        "\n",
        "            F = np.fft.fft2(gray_np)\n",
        "            Fshift = np.fft.fftshift(F)\n",
        "            magnitude = np.log1p(np.abs(Fshift))\n",
        "\n",
        "            gray_norm = (gray_np - gray_np.min()) / (gray_np.max() - gray_np.min() + 1e-8)\n",
        "            mag_norm = (magnitude - magnitude.min()) / (magnitude.max() - magnitude.min() + 1e-8)\n",
        "\n",
        "            two_channel = np.stack([gray_norm, mag_norm], axis=0)\n",
        "            return torch.tensor(two_channel, dtype=torch.float32)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"error: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "# ---------------------- 损坏数据集处理 ----------------------\n",
        "class SafeImageFolder(datasets.ImageFolder):  # 这边是因为数据集有问题  用来处理损坏的数据集  后面修正数据集后可以删掉\n",
        "    def __getitem__(self, index):\n",
        "        try:\n",
        "            path, _ = self.imgs[index]\n",
        "            img = self.loader(path)\n",
        "            if self.transform is not None:\n",
        "                img = self.transform(img)\n",
        "            return img, self.targets[index]\n",
        "        except Exception as e:\n",
        "            print(f\"error file: {self.imgs[index][0]} - {str(e)}\")\n",
        "            return None\n",
        "\n",
        "class FilteredDataset(Dataset):\n",
        "    def __init__(self, original_dataset):\n",
        "        self.original_dataset = original_dataset\n",
        "        self.valid_indices = []\n",
        "\n",
        "        for i in range(len(original_dataset)):\n",
        "            data = original_dataset[i]\n",
        "            if data is not None and data[0] is not None:\n",
        "                self.valid_indices.append(i)\n",
        "\n",
        "        if len(self.valid_indices) == 0:\n",
        "            raise RuntimeError(\"empty\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.original_dataset[self.valid_indices[i]]\n",
        "\n",
        "# ---------------------- 数据增强 ----------------------\n",
        "def Build(image_size=128):\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.Resize(int(image_size*1.2)),\n",
        "        transforms.RandomCrop(image_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.Lambda(lambda x: x),\n",
        "       FFT(image_size)\n",
        "    ])\n",
        "\n",
        "    val_transforms = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.Lambda(lambda x: x),\n",
        "       FFT(image_size)\n",
        "    ])\n",
        "\n",
        "    return train_transforms, val_transforms\n",
        "\n",
        "# ---------------------- 模型 ----------------------\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(2, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout2d(0.3)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128 * 16 * 16, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# ---------------------- 训练流程 ----------------------\n",
        "def main():\n",
        "    # 配置参数\n",
        "    DATA_DIR = \"dataset/513/\" # 替换成自己的路径\n",
        "    IMAGE_SIZE = 128\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCHS = 10 # 50轮跑不动就跑个10轮 20轮的就行\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_trans, val_trans = Build(IMAGE_SIZE)\n",
        "    full_dataset = SafeImageFolder(DATA_DIR, transform=train_trans)\n",
        "\n",
        "    filtered_dataset = FilteredDataset(full_dataset)\n",
        "\n",
        "    # 8 : 2 (train && val)\n",
        "    train_size = int(0.8 * len(filtered_dataset))\n",
        "    val_size = len(filtered_dataset) - train_size\n",
        "    train_set, val_set = random_split(filtered_dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_set, BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_set, BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    model = CNN(num_classes=len(full_dataset.classes)).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    steps = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        train_loss, correct = 0.0, 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss, val_correct = 0.0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "                outputs = model(inputs)\n",
        "                val_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_acc = correct / len(train_loader.dataset)\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_acc = val_correct / len(val_loader.dataset)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            steps = 0\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        else:\n",
        "            steps += 1\n",
        "            if steps >= 7:\n",
        "                print(f\"early stop\")\n",
        "                break\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
        "              f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "qdQ8A-wsjnuI",
        "outputId": "42a42265-637c-4a34-ad4f-d241b330ad7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'dataset/513/'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4587aaa42143>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-4587aaa42143>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mtrain_trans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mfull_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSafeImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_trans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mfiltered_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFilteredDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/513/'"
          ]
        }
      ]
    }
  ]
}